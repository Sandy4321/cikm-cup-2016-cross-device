{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cPickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_json(line):\n",
    "    d = json.loads(line)\n",
    "    user_facts = []\n",
    "    for f in d['facts']:\n",
    "        fid = f['fid'] - 1\n",
    "        ts = f['ts']\n",
    "        if ts > 1000000000000000:\n",
    "            user_facts.append((fid, ts / 1000))\n",
    "        else:\n",
    "            user_facts.append((fid, ts))\n",
    "    return user_facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "with open('tmp/df_train.bin', 'rb') as f:\n",
    "    df_train = cPickle.load(f)\n",
    "\n",
    "train_users = set(df_train.user_1) | set(df_train.user_2) \n",
    "train_idx = sorted(train_users)\n",
    "\n",
    "components = []\n",
    "uid_to_others = {}\n",
    "for _, group in tqdm(df_train.groupby('component')):\n",
    "    users = set(group.user_1) | set(group.user_2)\n",
    "    components.append(users)\n",
    "    for uid in users:\n",
    "        uid_to_others[uid] = users - {uid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000,) (30000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(120679, 120050)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "\n",
    "num_components = df_train.component.max()\n",
    "component_idx = np.arange(0, num_components)\n",
    "np.random.shuffle(component_idx)\n",
    "\n",
    "split = num_components / 2\n",
    "print component_idx[:split].shape, component_idx[split:].shape\n",
    "fold1_comps = set(component_idx[:split])\n",
    "fold2_comps = set(component_idx[split:])\n",
    "\n",
    "fold1 = df_train[df_train.component.isin(fold1_comps)]\n",
    "fold1_users = set(fold1.user_1) | set(fold1.user_2)\n",
    "fold2 = df_train[df_train.component.isin(fold2_comps)]\n",
    "fold2_users = set(fold2.user_1) | set(fold2.user_2)\n",
    "\n",
    "len(fold1_users), len(fold2_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_1 = 1\n",
    "TRAIN_2 = 2\n",
    "TEST = 3\n",
    "\n",
    "def user_fold(uid):\n",
    "    if uid in fold1_users:\n",
    "        return TRAIN_1\n",
    "    if uid in fold2_users:\n",
    "        return TRAIN_2\n",
    "    return TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>address</th>\n",
       "      <th>param</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6N6J</td>\n",
       "      <td>6KKH 3FI</td>\n",
       "      <td>3HZT5</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FFNE</td>\n",
       "      <td>AR4 O78Q O78R</td>\n",
       "      <td>6XUD</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BC6L</td>\n",
       "      <td>30DFF</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KAV3</td>\n",
       "      <td>MI 7FWM0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7D7H</td>\n",
       "      <td>6N5M 6JQ0 ZNMS 6MEA</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  domain              address  param title\n",
       "0   6N6J             6KKH 3FI  3HZT5      \n",
       "1   FFNE        AR4 O78Q O78R   6XUD      \n",
       "2   BC6L                30DFF             \n",
       "3   KAV3             MI 7FWM0             \n",
       "4   7D7H  6N5M 6JQ0 ZNMS 6MEA             "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('tmp/df_urls.bin', 'rb') as f:\n",
    "    df_urls = cPickle.load(f)\n",
    "df_urls.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "es_host = '172.17.0.2'\n",
    "es = Elasticsearch(host=es_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'acknowledged': True}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# es.indices.delete(index='user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from elasticsearch_dsl.connections import connections\n",
    "from elasticsearch_dsl import Mapping, String, Nested, Integer, Boolean\n",
    "from elasticsearch_dsl import analyzer, tokenizer\n",
    "\n",
    "whitespace_analyzer = analyzer('whitespace_analyzer', tokenizer=tokenizer('whitespace'))\n",
    "con = connections.create_connection(host=es_host)\n",
    "\n",
    "mapping = Mapping('user_log')\n",
    "\n",
    "fact = Nested(multi=True, include_in_parent=True)\n",
    "fact.field('domain', String(analyzer=whitespace_analyzer, ))\n",
    "fact.field('address', String(analyzer=whitespace_analyzer))\n",
    "fact.field('param', String(analyzer=whitespace_analyzer))\n",
    "fact.field('title', String(analyzer=whitespace_analyzer))\n",
    "\n",
    "mapping.field('fact', fact)\n",
    "mapping.field('fold', Integer(index='not_analyzed'))\n",
    "\n",
    "mapping.save('user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools \n",
    "\n",
    "def chunk_iterator(iterator, size):\n",
    "    while 1:\n",
    "        batch = list(itertools.islice(iterator, size))\n",
    "        if batch:\n",
    "            yield batch\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "uid_idx = 0\n",
    "\n",
    "with open('../data/facts.json', 'r') as fact_file:\n",
    "    lines = iter(fact_file)\n",
    "\n",
    "    for chunk in tqdm(chunk_iterator(lines, 100)):\n",
    "        actions = []\n",
    "\n",
    "        for line in chunk:\n",
    "            log = read_json(line)\n",
    "            facts, _ = zip(*log)\n",
    "            facts = df_urls.iloc[list(facts)]\n",
    "            user = {\n",
    "                'fact': facts.to_dict(orient='records'), \n",
    "                'fold': user_fold(uid_idx),\n",
    "            }\n",
    "            action = {'_id': uid_idx, '_index': 'user', '_type': 'user_log', '_source': user}\n",
    "            actions.append(action)\n",
    "            uid_idx = uid_idx + 1\n",
    "\n",
    "        helpers.bulk(es, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_similar(uid, limit=10):\n",
    "    query = {\n",
    "        'query': {\n",
    "            'filtered': {\n",
    "                'query': {\n",
    "                    'more_like_this': {\n",
    "                        'like': {\n",
    "                            '_index': 'user',\n",
    "                            '_type': 'user_log',\n",
    "                            '_id': uid,\n",
    "                        },\n",
    "                        'max_query_terms': 10,\n",
    "                        'fields': ['fact.domain', 'fact.address', 'fact.param', 'fact.title^2'],\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        'filter': {\n",
    "            'bool': {\n",
    "                'must': [{\n",
    "                    'term': {\n",
    "                        'fold': user_fold(uid),\n",
    "                    },\n",
    "                }],\n",
    "                \n",
    "            }\n",
    "        },\n",
    "        'fields': ['_id'],\n",
    "        'size': limit,\n",
    "    }\n",
    "\n",
    "    res = es.search(index='user', doc_type='user_log', body=query)\n",
    "    hits = res['hits']['hits']\n",
    "    return [(int(d['_id']), d['_score']) for d in hits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "random_users = set(np.random.choice(list(train_users), size=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "text/plain": [
       "(1244, 2059)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved = 0\n",
    "relevant = 0\n",
    "\n",
    "for uid in tqdm(random_users):\n",
    "    similar = find_similar(uid, limit=25)\n",
    "    others_found = {u for (u, _) in similar}\n",
    "    others_truth = uid_to_others[uid]\n",
    "    retrieved = retrieved + len(others_found & others_truth) \n",
    "    relevant = relevant + len(others_truth)\n",
    "\n",
    "retrieved, relevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ES Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('tmp/train_test_users.bin', 'rb') as f:\n",
    "    _, test_idx = cPickle.load(f)\n",
    "    test_idx = list(test_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "test_pairs = []\n",
    "\n",
    "for uid in tqdm(test_idx):\n",
    "    similar = find_similar(uid, limit=25)\n",
    "    test_pairs.extend((uid, u, s) for (u, s) in similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_pairs = sorted(test_pairs, key=lambda (a, b, s): s, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('tmp/idx_to_uid.bin', 'rb') as f:\n",
    "    idx_to_uid = cPickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count = 215307\n",
    "with open('submission.txt','w') as f_out:\n",
    "    seen = set()\n",
    "    for uid1, uid2, score in test_pairs:\n",
    "        if (uid1, uid2) in seen or (uid2, uid1) in seen:\n",
    "            continue\n",
    "        seen.add((uid1, uid2))\n",
    "\n",
    "        f_out.write(\"%s,%s\\n\" % (idx_to_uid[uid1], idx_to_uid[uid2]))\n",
    "        count = count - 1\n",
    "        if count <= 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!zip -r submission.txt.zip submission.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
